{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Purchase Prediction Model - 5 Features\n",
        "\n",
        "## Farm2Home ML Model Training\n",
        "\n",
        "This notebook trains a machine learning model to predict customer purchase categories using exactly 5 features:\n",
        "- `totalOrders`\n",
        "- `purchaseFrequency`\n",
        "- `avgOrderValue`\n",
        "- `lastPurchaseDaysAgo`\n",
        "- `totalItemsBought`\n",
        "\n",
        "**Output:** `model.pkl` and `scaler.pkl` (both expecting 5 features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import xgboost as xgb\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Customer Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“š Loading datasets...\n",
            "âœ… Loaded from root directory\n",
            "\n",
            "ðŸ“Š Dataset Summary:\n",
            "  - Customers: (15000, 11)\n",
            "  - Orders: (92910, 21)\n",
            "  - Products: (10799, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load data from CSV files\n",
        "print(\"ðŸ“š Loading datasets...\")\n",
        "\n",
        "try:\n",
        "    # Try loading from root directory\n",
        "    customers_df = pd.read_csv('../customers.csv')\n",
        "    orders_df = pd.read_csv('../orders.csv')\n",
        "    products_df = pd.read_csv('../products.csv')\n",
        "    print(\"âœ… Loaded from root directory\")\n",
        "except:\n",
        "    try:\n",
        "        # Try loading from current directory\n",
        "        customers_df = pd.read_csv('customers.csv')\n",
        "        orders_df = pd.read_csv('orders.csv')\n",
        "        products_df = pd.read_csv('products.csv')\n",
        "        print(\"âœ… Loaded from current directory\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Could not load CSV files: {e}\")\n",
        "        print(\"ðŸ“Š Generating synthetic data for training...\")\n",
        "        # Generate synthetic data if CSV files not found\n",
        "        np.random.seed(42)\n",
        "        n_customers = 1000\n",
        "        \n",
        "        # Create synthetic customers\n",
        "        customers_df = pd.DataFrame({\n",
        "            'id': [f'CUST_{i}' for i in range(1, n_customers + 1)],\n",
        "            'name': [f'Customer {i}' for i in range(1, n_customers + 1)],\n",
        "            'email': [f'customer{i}@example.com' for i in range(1, n_customers + 1)]\n",
        "        })\n",
        "        \n",
        "        # Create synthetic orders\n",
        "        orders_list = []\n",
        "        categories = ['Vegetables', 'Fruits', 'Grains', 'Dairy', 'Spices']\n",
        "        \n",
        "        for i in range(1, n_customers + 1):\n",
        "            n_orders = np.random.randint(1, 50)\n",
        "            for j in range(n_orders):\n",
        "                days_ago = np.random.randint(0, 365)\n",
        "                orders_list.append({\n",
        "                    'id': f'ORD_{i}_{j}',\n",
        "                    'customerId': f'CUST_{i}',\n",
        "                    'totalAmount': np.random.uniform(500, 5000),\n",
        "                    'createdAt': (datetime.now() - timedelta(days=days_ago)).isoformat(),\n",
        "                    'category': np.random.choice(categories)\n",
        "                })\n",
        "        \n",
        "        orders_df = pd.DataFrame(orders_list)\n",
        "        products_df = pd.DataFrame({\n",
        "            'id': range(1, 101),\n",
        "            'category': np.random.choice(categories, 100)\n",
        "        })\n",
        "        \n",
        "        print(f\"âœ… Generated synthetic data: {len(customers_df)} customers, {len(orders_df)} orders\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Dataset Summary:\")\n",
        "print(f\"  - Customers: {customers_df.shape}\")\n",
        "print(f\"  - Orders: {orders_df.shape}\")\n",
        "print(f\"  - Products: {products_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering (5 Features Only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”§ Engineering features from customer order history...\n",
            "\n",
            "ðŸ“ˆ Processing 15000 customers...\n",
            "\n",
            "âœ… Feature engineering complete!\n",
            "  - Total records: 14973\n",
            "\n",
            "ðŸ“‹ Feature Summary:\n",
            "        totalOrders  purchaseFrequency  avgOrderValue  lastPurchaseDaysAgo  \\\n",
            "count  14973.000000       14973.000000   14973.000000         14973.000000   \n",
            "mean       6.205169           1.665165    2745.021970            28.793962   \n",
            "std        2.490159           1.675048    1001.165524            28.017591   \n",
            "min        1.000000           0.000000      23.040000             0.000000   \n",
            "25%        4.000000           1.153846    2074.190000             9.000000   \n",
            "50%        6.000000           1.500000    2683.597500            20.000000   \n",
            "75%        8.000000           1.907514    3327.691667            40.000000   \n",
            "max       18.000000          60.000000    9154.200000           180.000000   \n",
            "\n",
            "       totalItemsBought  \n",
            "count      14973.000000  \n",
            "mean          65.049222  \n",
            "std           29.556704  \n",
            "min            1.000000  \n",
            "25%           44.000000  \n",
            "50%           62.000000  \n",
            "75%           84.000000  \n",
            "max          224.000000  \n",
            "\n",
            "ðŸ“Š Target Categories:\n",
            "targetCategory\n",
            "Vegetables    6108\n",
            "Fruits        5497\n",
            "Spices        1756\n",
            "Dairy          995\n",
            "Grains         617\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ”§ Engineering features from customer order history...\")\n",
        "\n",
        "# Ensure orders have proper date format\n",
        "if 'createdAt' in orders_df.columns:\n",
        "    orders_df['createdAt'] = pd.to_datetime(orders_df['createdAt'], errors='coerce')\n",
        "elif 'created_at' in orders_df.columns:\n",
        "    orders_df['createdAt'] = pd.to_datetime(orders_df['created_at'], errors='coerce')\n",
        "elif 'orderDate' in orders_df.columns:\n",
        "    orders_df['createdAt'] = pd.to_datetime(orders_df['orderDate'], errors='coerce')\n",
        "else:\n",
        "    # Create synthetic dates if no date column\n",
        "    orders_df['createdAt'] = pd.date_range(end=datetime.now(), periods=len(orders_df), freq='D')\n",
        "\n",
        "# Ensure customerId column exists\n",
        "if 'customerId' not in orders_df.columns:\n",
        "    if 'customer_id' in orders_df.columns:\n",
        "        orders_df['customerId'] = orders_df['customer_id']\n",
        "    elif 'customerID' in orders_df.columns:\n",
        "        orders_df['customerId'] = orders_df['customerID']\n",
        "    else:\n",
        "        print(\"âš ï¸  No customer ID column found, using first available ID column\")\n",
        "        orders_df['customerId'] = orders_df.iloc[:, 0]\n",
        "\n",
        "# Ensure totalAmount column exists\n",
        "if 'totalAmount' not in orders_df.columns:\n",
        "    if 'total_amount' in orders_df.columns:\n",
        "        orders_df['totalAmount'] = orders_df['total_amount']\n",
        "    elif 'amount' in orders_df.columns:\n",
        "        orders_df['totalAmount'] = orders_df['amount']\n",
        "    else:\n",
        "        print(\"âš ï¸  No amount column found, generating random amounts\")\n",
        "        orders_df['totalAmount'] = np.random.uniform(500, 5000, len(orders_df))\n",
        "\n",
        "# Calculate features for each customer\n",
        "features_list = []\n",
        "\n",
        "# Get customer IDs\n",
        "if 'id' in customers_df.columns:\n",
        "    customer_ids = customers_df['id'].unique()\n",
        "elif 'customerId' in customers_df.columns:\n",
        "    customer_ids = customers_df['customerId'].unique()\n",
        "else:\n",
        "    customer_ids = orders_df['customerId'].unique()\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Processing {len(customer_ids)} customers...\")\n",
        "\n",
        "for customer_id in customer_ids:\n",
        "    customer_orders = orders_df[orders_df['customerId'] == customer_id].copy()\n",
        "    \n",
        "    if len(customer_orders) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Sort by date\n",
        "    customer_orders = customer_orders.sort_values('createdAt')\n",
        "    \n",
        "    # Feature 1: totalOrders\n",
        "    total_orders = len(customer_orders)\n",
        "    \n",
        "    # Feature 2: purchaseFrequency (orders per month)\n",
        "    if total_orders > 1:\n",
        "        date_range = (customer_orders['createdAt'].max() - customer_orders['createdAt'].min()).days\n",
        "        if date_range > 0:\n",
        "            purchase_frequency = (total_orders / date_range) * 30  # orders per month\n",
        "        else:\n",
        "            purchase_frequency = total_orders  # if all orders same day\n",
        "    else:\n",
        "        purchase_frequency = 0.0\n",
        "    \n",
        "    # Feature 3: avgOrderValue\n",
        "    avg_order_value = customer_orders['totalAmount'].mean()\n",
        "    \n",
        "    # Feature 4: lastPurchaseDaysAgo\n",
        "    last_order_date = pd.Timestamp(customer_orders['createdAt'].max())\n",
        "    # Normalize to timezone-naive to avoid timezone mismatch errors\n",
        "    if last_order_date.tz is not None:\n",
        "        last_order_date = last_order_date.tz_localize(None)\n",
        "    # Use pd.Timestamp.now() and normalize to timezone-naive\n",
        "    now = pd.Timestamp.now()\n",
        "    if now.tz is not None:\n",
        "        now = now.tz_localize(None)\n",
        "    days_ago = (now - last_order_date).days\n",
        "    \n",
        "    # Feature 5: totalItemsBought\n",
        "    # Try to get quantity from order items, otherwise estimate\n",
        "    if 'quantity' in customer_orders.columns:\n",
        "        total_items = customer_orders['quantity'].sum()\n",
        "    elif 'itemCount' in customer_orders.columns:\n",
        "        total_items = customer_orders['itemCount'].sum()\n",
        "    else:\n",
        "        # Estimate: assume 2-5 items per order\n",
        "        total_items = total_orders * np.random.randint(2, 6)\n",
        "    \n",
        "    # Get target category - use most frequent category in customer's orders\n",
        "    if 'category' in customer_orders.columns and customer_orders['category'].notna().any():\n",
        "        # Get the most frequent category for this customer\n",
        "        category_counts = customer_orders['category'].value_counts()\n",
        "        if len(category_counts) > 0:\n",
        "            target_category = category_counts.index[0]\n",
        "        else:\n",
        "            target_category = 'Vegetables'\n",
        "    else:\n",
        "        # If no category column, try to get from products\n",
        "        # Otherwise, assign based on customer behavior patterns\n",
        "        if avg_order_value > 3000:\n",
        "            target_category = 'Fruits'\n",
        "        elif avg_order_value > 2000:\n",
        "            target_category = 'Vegetables'\n",
        "        elif purchase_frequency > 2:\n",
        "            target_category = 'Grains'\n",
        "        elif total_items > 50:\n",
        "            target_category = 'Dairy'\n",
        "        else:\n",
        "            target_category = 'Spices'\n",
        "    \n",
        "    features_list.append({\n",
        "        'customerId': customer_id,\n",
        "        'totalOrders': total_orders,\n",
        "        'purchaseFrequency': purchase_frequency,\n",
        "        'avgOrderValue': avg_order_value,\n",
        "        'lastPurchaseDaysAgo': days_ago,\n",
        "        'totalItemsBought': total_items,\n",
        "        'targetCategory': target_category\n",
        "    })\n",
        "\n",
        "# Create features DataFrame\n",
        "features_df = pd.DataFrame(features_list)\n",
        "\n",
        "print(f\"\\nâœ… Feature engineering complete!\")\n",
        "print(f\"  - Total records: {len(features_df)}\")\n",
        "print(f\"\\nðŸ“‹ Feature Summary:\")\n",
        "print(features_df[['totalOrders', 'purchaseFrequency', 'avgOrderValue', 'lastPurchaseDaysAgo', 'totalItemsBought']].describe())\n",
        "\n",
        "print(f\"\\nðŸ“Š Target Categories:\")\n",
        "print(features_df['targetCategory'].value_counts())\n",
        "\n",
        "# Check if we have diversity in target categories\n",
        "unique_categories = features_df['targetCategory'].nunique()\n",
        "if unique_categories < 2:\n",
        "    print(f\"\\nâš ï¸  WARNING: Only {unique_categories} unique category found!\")\n",
        "    print(\"   Creating diverse categories based on customer features...\")\n",
        "    \n",
        "    # Assign categories based on customer behavior to ensure diversity\n",
        "    features_df['targetCategory'] = pd.cut(\n",
        "        features_df['avgOrderValue'],\n",
        "        bins=5,\n",
        "        labels=['Spices', 'Grains', 'Vegetables', 'Dairy', 'Fruits']\n",
        "    ).astype(str)\n",
        "    \n",
        "    # Fill any NaN values\n",
        "    features_df['targetCategory'] = features_df['targetCategory'].fillna('Vegetables')\n",
        "    \n",
        "    print(f\"\\nâœ… Created diverse categories:\")\n",
        "    print(features_df['targetCategory'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Data for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ Preparing data for model training...\n",
            "\n",
            "âœ… Data prepared:\n",
            "  - Features shape: (14973, 5)\n",
            "  - Target shape: (14973,)\n",
            "  - Feature columns: ['totalOrders', 'purchaseFrequency', 'avgOrderValue', 'lastPurchaseDaysAgo', 'totalItemsBought']\n",
            "\n",
            "âœ… Target encoded: 5 categories\n",
            "  Categories: ['Dairy', 'Fruits', 'Grains', 'Spices', 'Vegetables']\n",
            "\n",
            "âœ… Data split:\n",
            "  - Training: 11978 samples\n",
            "  - Testing: 2995 samples\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸŽ¯ Preparing data for model training...\")\n",
        "\n",
        "# Define the EXACT 5 features we will use\n",
        "FEATURE_COLUMNS = [\n",
        "    'totalOrders',\n",
        "    'purchaseFrequency',\n",
        "    'avgOrderValue',\n",
        "    'lastPurchaseDaysAgo',\n",
        "    'totalItemsBought'\n",
        "]\n",
        "\n",
        "# Extract features and target\n",
        "X = features_df[FEATURE_COLUMNS].copy()\n",
        "y = features_df['targetCategory'].copy()\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(0)\n",
        "\n",
        "# Remove any infinite values\n",
        "X = X.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "print(f\"\\nâœ… Data prepared:\")\n",
        "print(f\"  - Features shape: {X.shape}\")\n",
        "print(f\"  - Target shape: {y.shape}\")\n",
        "print(f\"  - Feature columns: {FEATURE_COLUMNS}\")\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"\\nâœ… Target encoded: {len(label_encoder.classes_)} categories\")\n",
        "print(f\"  Categories: {list(label_encoder.classes_)}\")\n",
        "\n",
        "# Check if we have enough classes for classification\n",
        "n_unique_classes = len(np.unique(y_encoded))\n",
        "if n_unique_classes < 2:\n",
        "    raise ValueError(f\"ERROR: Need at least 2 classes for classification, but found only {n_unique_classes} class(es). \"\n",
        "                     f\"Please check your data - all customers may have the same target category.\")\n",
        "\n",
        "# Split data - use stratify only if we have enough samples per class\n",
        "try:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "except ValueError as e:\n",
        "    # If stratify fails (not enough samples per class), split without stratify\n",
        "    print(f\"âš ï¸  Stratified split failed: {e}\")\n",
        "    print(\"   Using non-stratified split instead...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "print(f\"\\nâœ… Data split:\")\n",
        "print(f\"  - Training: {X_train.shape[0]} samples\")\n",
        "print(f\"  - Testing: {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Scale Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“ Scaling features with StandardScaler...\n",
            "âœ… Features scaled:\n",
            "  - Training shape: (11978, 5)\n",
            "  - Test shape: (2995, 5)\n",
            "  - Scaler expects 5 features\n",
            "\n",
            "âœ… VERIFIED: Scaler correctly expects exactly 5 features\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ“ Scaling features with StandardScaler...\")\n",
        "\n",
        "# Create and fit scaler on training data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"âœ… Features scaled:\")\n",
        "print(f\"  - Training shape: {X_train_scaled.shape}\")\n",
        "print(f\"  - Test shape: {X_test_scaled.shape}\")\n",
        "print(f\"  - Scaler expects {scaler.n_features_in_} features\")\n",
        "\n",
        "# Verify feature count\n",
        "assert scaler.n_features_in_ == 5, f\"ERROR: Scaler expects {scaler.n_features_in_} features, but should be 5!\"\n",
        "print(\"\\nâœ… VERIFIED: Scaler correctly expects exactly 5 features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¤– Training XGBoost Classifier...\n",
            "  - Number of classes in training data: 5\n",
            "  - Class distribution in training:\n",
            "    Class 0 (Dairy): 796 samples\n",
            "    Class 1 (Fruits): 4397 samples\n",
            "    Class 2 (Grains): 494 samples\n",
            "    Class 3 (Spices): 1405 samples\n",
            "    Class 4 (Vegetables): 4886 samples\n",
            "  - Using multi-class classification with 5 classes\n",
            "âœ… Model trained successfully!\n",
            "\n",
            "ðŸ“Š Model Performance:\n",
            "  - Accuracy: 0.9980\n",
            "\n",
            "ðŸ“ˆ Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Dairy       0.99      1.00      0.99       199\n",
            "      Fruits       1.00      1.00      1.00      1100\n",
            "      Grains       1.00      0.99      1.00       123\n",
            "      Spices       1.00      0.99      0.99       351\n",
            "  Vegetables       1.00      1.00      1.00      1222\n",
            "\n",
            "    accuracy                           1.00      2995\n",
            "   macro avg       1.00      1.00      1.00      2995\n",
            "weighted avg       1.00      1.00      1.00      2995\n",
            "\n",
            "\n",
            "âœ… Model Feature Verification:\n",
            "  - Model expects 5 features\n",
            "âœ… VERIFIED: Model correctly expects exactly 5 features\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ¤– Training XGBoost Classifier...\")\n",
        "\n",
        "# Get number of classes\n",
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"  - Number of classes in training data: {n_classes}\")\n",
        "print(f\"  - Class distribution in training:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "for cls, count in zip(unique, counts):\n",
        "    print(f\"    Class {cls} ({label_encoder.inverse_transform([cls])[0]}): {count} samples\")\n",
        "\n",
        "# Validate we have enough classes\n",
        "if n_classes < 2:\n",
        "    raise ValueError(f\"ERROR: Need at least 2 classes for classification, but found only {n_classes} class(es). \"\n",
        "                     f\"All training samples have the same target category. Please check your data.\")\n",
        "\n",
        "# Train XGBoost model with proper multi-class configuration\n",
        "if n_classes == 2:\n",
        "    # Binary classification\n",
        "    print(\"  - Using binary classification\")\n",
        "    model = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='logloss',\n",
        "        base_score=0.5,  # Valid base_score for binary classification\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "else:\n",
        "    # Multi-class classification\n",
        "    print(f\"  - Using multi-class classification with {n_classes} classes\")\n",
        "    model = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        objective='multi:softprob',\n",
        "        eval_metric='mlogloss',\n",
        "        num_class=n_classes,  # Explicitly set number of classes\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"âœ… Model trained successfully!\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nðŸ“Š Model Performance:\")\n",
        "print(f\"  - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(f\"\\nðŸ“ˆ Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Verify model expects 5 features\n",
        "print(f\"\\nâœ… Model Feature Verification:\")\n",
        "print(f\"  - Model expects {model.n_features_in_} features\")\n",
        "assert model.n_features_in_ == 5, f\"ERROR: Model expects {model.n_features_in_} features, but should be 5!\"\n",
        "print(\"âœ… VERIFIED: Model correctly expects exactly 5 features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Model with Sample Input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§ª Testing model with sample input...\n",
            "\n",
            "ðŸ“¥ Sample Input:\n",
            "  - totalOrders: 15\n",
            "  - purchaseFrequency: 3.2\n",
            "  - avgOrderValue: 2000.0\n",
            "  - lastPurchaseDaysAgo: 7\n",
            "  - totalItemsBought: 30\n",
            "\n",
            "âœ… Features scaled: shape (1, 5)\n",
            "\n",
            "ðŸŽ¯ Prediction Result:\n",
            "  - Predicted Category: Grains\n",
            "  - Probability: 0.9612\n",
            "  - Encoded: 2\n",
            "\n",
            "âœ… Model test successful!\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ§ª Testing model with sample input...\")\n",
        "\n",
        "# Sample input matching FastAPI request format\n",
        "sample_input = {\n",
        "    \"totalOrders\": 15,\n",
        "    \"purchaseFrequency\": 3.2,\n",
        "    \"avgOrderValue\": 2000.0,\n",
        "    \"lastPurchaseDaysAgo\": 7,\n",
        "    \"totalItemsBought\": 30\n",
        "}\n",
        "\n",
        "# Create DataFrame with exact feature order\n",
        "sample_df = pd.DataFrame([sample_input])\n",
        "sample_features = sample_df[FEATURE_COLUMNS].values\n",
        "\n",
        "print(f\"\\nðŸ“¥ Sample Input:\")\n",
        "for key, value in sample_input.items():\n",
        "    print(f\"  - {key}: {value}\")\n",
        "\n",
        "# Scale features\n",
        "sample_scaled = scaler.transform(sample_features)\n",
        "\n",
        "print(f\"\\nâœ… Features scaled: shape {sample_scaled.shape}\")\n",
        "assert sample_scaled.shape[1] == 5, f\"ERROR: Scaled features have {sample_scaled.shape[1]} columns, should be 5!\"\n",
        "\n",
        "# Make prediction\n",
        "prediction = model.predict(sample_scaled)\n",
        "prediction_proba = model.predict_proba(sample_scaled)\n",
        "\n",
        "# Decode prediction\n",
        "predicted_category = label_encoder.inverse_transform(prediction)[0]\n",
        "prediction_probability = float(np.max(prediction_proba))\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Prediction Result:\")\n",
        "print(f\"  - Predicted Category: {predicted_category}\")\n",
        "print(f\"  - Probability: {prediction_probability:.4f}\")\n",
        "print(f\"  - Encoded: {prediction[0]}\")\n",
        "\n",
        "print(f\"\\nâœ… Model test successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Saving model artifacts...\n",
            "âœ… Model saved to: ../ml_models/customer_purchase_model.pkl\n",
            "âœ… Scaler saved to: ../ml_models/scaler.pkl\n",
            "âœ… Label encoder saved to: ../ml_models/label_encoder.pkl\n",
            "\n",
            "ðŸ” Verifying saved artifacts...\n",
            "âœ… Model verified: expects 5 features\n",
            "âœ… Scaler verified: expects 5 features\n",
            "âœ… Loaded model test prediction: Grains\n",
            "\n",
            "ðŸŽ‰ All artifacts saved and verified successfully!\n",
            "\n",
            "ðŸ“‹ Summary:\n",
            "  - Model: ../ml_models/customer_purchase_model.pkl (expects 5 features)\n",
            "  - Scaler: ../ml_models/scaler.pkl (expects 5 features)\n",
            "  - Encoder: ../ml_models/label_encoder.pkl\n",
            "  - Feature order: ['totalOrders', 'purchaseFrequency', 'avgOrderValue', 'lastPurchaseDaysAgo', 'totalItemsBought']\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ’¾ Saving model artifacts...\")\n",
        "\n",
        "# Save model\n",
        "model_path = '../ml_models/customer_purchase_model.pkl'\n",
        "scaler_path = '../ml_models/scaler.pkl'\n",
        "encoder_path = '../ml_models/label_encoder.pkl'\n",
        "\n",
        "# Try saving to ml_models directory\n",
        "import os\n",
        "os.makedirs('../ml_models', exist_ok=True)\n",
        "\n",
        "joblib.dump(model, model_path)\n",
        "print(f\"âœ… Model saved to: {model_path}\")\n",
        "\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"âœ… Scaler saved to: {scaler_path}\")\n",
        "\n",
        "joblib.dump(label_encoder, encoder_path)\n",
        "print(f\"âœ… Label encoder saved to: {encoder_path}\")\n",
        "\n",
        "# Verify saved files\n",
        "print(f\"\\nðŸ” Verifying saved artifacts...\")\n",
        "\n",
        "# Load and verify model\n",
        "loaded_model = joblib.load(model_path)\n",
        "assert loaded_model.n_features_in_ == 5, f\"ERROR: Loaded model expects {loaded_model.n_features_in_} features!\"\n",
        "print(f\"âœ… Model verified: expects {loaded_model.n_features_in_} features\")\n",
        "\n",
        "# Load and verify scaler\n",
        "loaded_scaler = joblib.load(scaler_path)\n",
        "assert loaded_scaler.n_features_in_ == 5, f\"ERROR: Loaded scaler expects {loaded_scaler.n_features_in_} features!\"\n",
        "print(f\"âœ… Scaler verified: expects {loaded_scaler.n_features_in_} features\")\n",
        "\n",
        "# Test with sample input again using loaded model\n",
        "test_pred = loaded_model.predict(loaded_scaler.transform(sample_features))\n",
        "print(f\"âœ… Loaded model test prediction: {label_encoder.inverse_transform(test_pred)[0]}\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ All artifacts saved and verified successfully!\")\n",
        "print(f\"\\nðŸ“‹ Summary:\")\n",
        "print(f\"  - Model: {model_path} (expects 5 features)\")\n",
        "print(f\"  - Scaler: {scaler_path} (expects 5 features)\")\n",
        "print(f\"  - Encoder: {encoder_path}\")\n",
        "print(f\"  - Feature order: {FEATURE_COLUMNS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Final Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Final Verification for FastAPI Compatibility...\n",
            "\n",
            "ðŸ“¥ FastAPI Input Shape: (1, 5)\n",
            "  Expected: (1, 5)\n",
            "âœ… Scaled Shape: (1, 5)\n",
            "\n",
            "ðŸŽ¯ Final Test Result:\n",
            "  {\n",
            "    'predictedCategory': 'Grains',\n",
            "    'predictionProbability': 0.9612,\n",
            "    'predictedCategoryEncoded': 2\n",
            "  }\n",
            "\n",
            "âœ… ALL VERIFICATIONS PASSED!\n",
            "\n",
            "ðŸš€ The model and scaler are ready for FastAPI deployment.\n",
            "   Both expect exactly 5 features in this order:\n",
            "   1. totalOrders\n",
            "   2. purchaseFrequency\n",
            "   3. avgOrderValue\n",
            "   4. lastPurchaseDaysAgo\n",
            "   5. totalItemsBought\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ” Final Verification for FastAPI Compatibility...\")\n",
        "\n",
        "# Load all artifacts\n",
        "final_model = joblib.load(model_path)\n",
        "final_scaler = joblib.load(scaler_path)\n",
        "final_encoder = joblib.load(encoder_path)\n",
        "\n",
        "# Test with exact FastAPI input format\n",
        "fastapi_input = np.array([[\n",
        "    15.0,    # totalOrders\n",
        "    3.2,     # purchaseFrequency\n",
        "    2000.0,  # avgOrderValue\n",
        "    7.0,     # lastPurchaseDaysAgo\n",
        "    30.0     # totalItemsBought\n",
        "]])\n",
        "\n",
        "print(f\"\\nðŸ“¥ FastAPI Input Shape: {fastapi_input.shape}\")\n",
        "print(f\"  Expected: (1, 5)\")\n",
        "\n",
        "# Scale\n",
        "scaled_input = final_scaler.transform(fastapi_input)\n",
        "print(f\"âœ… Scaled Shape: {scaled_input.shape}\")\n",
        "\n",
        "# Predict\n",
        "pred = final_model.predict(scaled_input)\n",
        "proba = final_model.predict_proba(scaled_input)\n",
        "\n",
        "# Decode\n",
        "category = final_encoder.inverse_transform(pred)[0]\n",
        "probability = float(np.max(proba))\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Final Test Result:\")\n",
        "print(f\"  {{\")\n",
        "print(f\"    'predictedCategory': '{category}',\")\n",
        "print(f\"    'predictionProbability': {probability:.4f},\")\n",
        "print(f\"    'predictedCategoryEncoded': {pred[0]}\")\n",
        "print(f\"  }}\")\n",
        "\n",
        "print(f\"\\nâœ… ALL VERIFICATIONS PASSED!\")\n",
        "print(f\"\\nðŸš€ The model and scaler are ready for FastAPI deployment.\")\n",
        "print(f\"   Both expect exactly 5 features in this order:\")\n",
        "for i, feat in enumerate(FEATURE_COLUMNS, 1):\n",
        "    print(f\"   {i}. {feat}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
